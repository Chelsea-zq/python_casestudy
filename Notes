import pandas as pd
import os
import time

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)

os.getcwd()
path = r'C:\Users\zhang\Desktop\inventory mapping'
os.chdir(path)

#批量处理Json文件

import pandas as pd
import numpy as np
import os
import json

path = r'C:\Users\zhang\Desktop\json'
filelist = os.listdir(path)

data_json = pd.DataFrame([])
data_json_all = pd.DataFrame([])

for json_file in filelist:
    with open(os.path.join(path,json_file),'r') as f:
        data = json.loads(f.read())
        data_json = pd.json_normalize(data,record_path=['items'],meta=['businessUnit'])
        data_json_all = pd.concat([data_json,data_json_all])

#聚合统计
data_json_all.onhandAvailableQuantity.sum()
data_json_all.groupby(['businessUnit','universalProductCode']).sum()

#批量处理Excel workbook中的所有sheet

import pandas as pd
import xlrd
import time
excel_name = r'C:\Users\zhang\Desktop\nike0531.xlsx'
wb = openpyxl.load_workbook(excel_name)
sheets = wb.sheetnames
excel_all = pd.DataFrame()


time_begin = time.time()

for i in range(len(sheets)):
    inv = pd.read_excel(excel_name,sheet_name=i,usecols = [3,4,5,6,7,9],engine='openpyxl')
    excel_all = pd.concat([inv,excel_all])
    
time_end = time.time()

time = time_end - time_begin

#批量处理文件夹中的所有Excel

import os
os.getcwd()      # 返回当前的工作目录
os.chdir(r'C:\Users\zhang\Desktop\机器学习\case001_基于随机森林回归算法的气温预测') # 修改当前的工作目录
import glob
import pandas as pd

paths = glob.glob('*.csv')
excel_all = pd.DataFrame()

for path in paths:
    excel_ = pd.read_csv(path)
    excel_all = pd.concat([excel_,excel_all])

#根据列名导出到一个workbook
writer = pd.ExcelWriter('output.xlsx')
for p in df['province'].unique():
    df.loc[df['province'] == p].to_excel(writer, sheet_name=p)
writer.save()

#导入大文件
reader = pd.read_csv('user.csv',encoding='gbk',low_memory=False,iterator=True)
loop = True
chunkSize = 10000000
chunks = []
while loop:
    try:
        chunk = reader.get_chunk(chunkSize)
        chunks.append(chunk)
    except StopIteration:
        loop = False
        print("Iteration is stopped.")
df = pd.concat(chunks, ignore_index=True)

#导入文本文件
shopping_list = "/Users/johnreid/Downloads/shopping_list.txt"
result = []
with open(shopping_list) as f:
    line = f.readline()
    while line:
        result.append(line.strip().split(" " ))
        line = f.readline()
f.close()
df_from_textfile = pd.DataFrame(results, columns = ["Item", "Quantity"])

#导入多个文件/文件夹
import glob
import pandas as pd
base_path = "/Users/johnreid/Downloads/bbcsport/"
genres = ["athletics", "cricket", "football", "rugby", "tennis"]
def read_and_split_file(filename):
    with open(filename, 'r', encoding="latin-1") as f:
        lines = f.readlines() # Get lines as a list of strings
        lines = list(map(str.strip, lines)) # Remove /n characters
        lines = list(filter(None, lines)) # Remove empty strings
    return lines
def get_df_from_genre(path, genre):
    files = glob.glob(path + genre + "/*.txt")
    titles = []
    subtitles = []
    bodies = []
    for f in files:
        lines = read_and_split_file(f)
        titles.append(lines[0]) # First line is the title
        subtitles.append(lines[1]) # Second line is the subtitle
        bodies.append(' '.join(lines[2:])) # Combine all the rest
    return(pd.DataFrame({
        'genre': genre,
        'title': titles,
        'subtitle': subtitles,
        'body': bodies
        })
    )
final_df = pd.concat([get_df_from_genre(base_path, g) for g in genres])
final_df

#远程数据库文件
host = "localhost"
database= "suppliers"
user = "postgres"
password = "SecurePas$1"
and then import it into your Python script as follows (you’ll also need the psychopg2 library):
import psycopg2
import config
conn = psycopg2.connect(
    host=config.host,
    database=config.database,
    user=config.user,
    password=config.password)
df1 = pd.read_sql_query("SELECT * FROM invoice", conn)
